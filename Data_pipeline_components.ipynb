{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "MLEPC2W3-1X",
        "MLEPC2W3-2X",
        "MLEPC2W3-3X",
        "MLEPC2W3-4X",
        "MLEPC2W3-5X",
        "MLEPC2W3-6X",
        "MLEPC2W3-7X",
        "MLEPC2W3-8X",
        "MLEPC2W3-9X",
        "MLEPC2W3-10X",
        "MLEPC2W3-11X",
        "MLEPC2W3-12X"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Data pipeline components.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saber-Hosseinzade/TensorFlowExtended_TFX/blob/main/Data_pipeline_components.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZXjh6D9nOUX"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HVVcPqlvKRh"
      },
      "source": [
        "# Restart the RunTime after completion of this section\n",
        "!pip install tfx==1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK6SyLhQP_s5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tfx\n",
        "\n",
        "# TFX components\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Transform\n",
        "from tfx.components import ImporterNode\n",
        "\n",
        "# TFX libraries\n",
        "import tensorflow_data_validation as tfdv\n",
        "import tensorflow_transform as tft\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "\n",
        "# For performing feature selection\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# For feature visualization\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "\n",
        "# Utilities\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "from  tfx.proto import example_gen_pb2\n",
        "from tfx.types import standard_artifacts\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "\n",
        "# To ignore warnings from TF\n",
        "#tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# For formatting print statements\n",
        "pp = pprint.PrettyPrinter()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsHcencfobKL"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Load the forest cover type dataset\n",
        "\n",
        "\n",
        "| Column Name | Variable Type | Units / Range | Description |\n",
        "| --------- | ------------ | ----- | ------------------- |\n",
        "| Elevation | quantitative |meters | Elevation in meters |\n",
        "| Aspect | quantitative | azimuth | Aspect in degrees azimuth |\n",
        "| Slope | quantitative | degrees | Slope in degrees |\n",
        "| Horizontal_Distance_To_Hydrology | quantitative | meters | Horz Dist to nearest surface water features |\n",
        "| Vertical_Distance_To_Hydrology | quantitative | meters | Vert Dist to nearest surface water features |\n",
        "| Horizontal_Distance_To_Roadways | quantitative | meters | Horz Dist to nearest roadway |\n",
        "| Hillshade_9am | quantitative | 0 to 255 index | Hillshade index at 9am, summer solstice |\n",
        "| Hillshade_Noon | quantitative | 0 to 255 index | Hillshade index at noon, summer soltice |\n",
        "| Hillshade_3pm | quantitative | 0 to 255 index | Hillshade index at 3pm, summer solstice |\n",
        "| Horizontal_Distance_To_Fire_Points | quantitative | meters | Horz Dist to nearest wildfire ignition points |\n",
        "| Wilderness_Area (4 binary columns) | qualitative | 0 (absence) or 1 (presence) | Wilderness area designation |\n",
        "| Soil_Type (40 binary columns) | qualitative | 0 (absence) or 1 (presence) | Soil Type designation |\n",
        "| Cover_Type (7 types) | integer | 1 to 7 | Forest Cover Type designation |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBJqC6-HfymZ"
      },
      "source": [
        "# Declare paths to the data\n",
        "data_dir = './data'\n",
        "training_dir = './data/training'\n",
        "training_file = './data/training/dataset.csv'\n",
        "\n",
        "!rm -rf pipeline\n",
        "!rm -rf data\n",
        "\n",
        "# Create the directory\n",
        "!mkdir -p {training_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-o0Pk8nf7FZ"
      },
      "source": [
        "# download the dataset\n",
        "!wget -nc https://storage.googleapis.com/workshop-datasets/covertype/full/dataset.csv -P {training_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0gqgbTF2Cgl"
      },
      "source": [
        "<a name='3'></a>\n",
        "## Feature Selection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6MverMGJwg8"
      },
      "source": [
        "df = pd.read_csv(training_file)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBhNY07BlD7z"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvmtVXtJeUOx"
      },
      "source": [
        "df_num = df.copy()\n",
        "\n",
        "cat_columns = ['Wilderness_Area', 'Soil_Type']\n",
        "\n",
        "label_column = ['Cover_Type']\n",
        "\n",
        "df_num.drop(cat_columns, axis=1, inplace=True)\n",
        "df_num.drop(label_column, axis=1, inplace=True)\n",
        "\n",
        "df_num.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ShO1olUeiUb"
      },
      "source": [
        "y = df[label_column].values\n",
        "\n",
        "X = df_num.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QlfkDyfGKDv"
      },
      "source": [
        "<a name='ex-1'></a>\n",
        "### Exercise 1: Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQgcpJqSenLq"
      },
      "source": [
        "skb = SelectKBest(score_func=f_classif, k=8)\n",
        "\n",
        "X_new = skb.fit_transform(X, y)\n",
        "\n",
        "features_mask = skb.get_support()\n",
        "\n",
        "reqd_cols = pd.DataFrame({'Columns': df_num.columns, 'Retain': features_mask})\n",
        "print(reqd_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A2jsRvhd-dR"
      },
      "source": [
        "# Set the paths to the reduced dataset\n",
        "training_dir_skb = f'{training_dir}/fselect'\n",
        "training_file_skb = f'{training_dir_skb}/dataset.csv'\n",
        "\n",
        "# Create the directory\n",
        "!mkdir -p {training_dir_skb}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3f_iNgne7Tk"
      },
      "source": [
        "# Get the feature names from SelectKBest\n",
        "skb_features = list(df_num.columns[features_mask])\n",
        "\n",
        "# Append the categorical and label columns\n",
        "skb_features = skb_features + cat_columns + label_column\n",
        "\n",
        "df_skb = df[skb_features]\n",
        "\n",
        "df_skb.to_csv(training_file_skb, index=False)\n",
        "\n",
        "df_skb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZES9v8ggpDv8"
      },
      "source": [
        "###  Interactive Context\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G-G-xLO3lkt"
      },
      "source": [
        "pipe_dir = './pipeline'\n",
        "\n",
        "context = InteractiveContext(pipeline_root=pipe_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_jT4_QvOJOb"
      },
      "source": [
        "\n",
        "### ExampleGen\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq8P5Fx3hSOB"
      },
      "source": [
        "# # NOTE: Uncomment and run this if you get an error saying there are different \n",
        "# # headers in the dataset. This is usually because of the notebook checkpoints saved in \n",
        "# # that folder.\n",
        "# !rm -rf {TRAINING_DIR}/.ipynb_checkpoints\n",
        "# !rm -rf {TRAINING_DIR_FSELECT}/.ipynb_checkpoints\n",
        "# !rm -rf {SERVING_DIR}/.ipynb_checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL3CZQcg3lku"
      },
      "source": [
        "example_gen = CsvExampleGen(input_base=training_dir_skb)\n",
        "\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz5G_wYnVsHP"
      },
      "source": [
        "\n",
        "### StatisticsGen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u2EOMOm3lkw"
      },
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "\n",
        "context.run(statistics_gen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exyb-VKD3lkw"
      },
      "source": [
        "# Display the results\n",
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6V9o7XnXKkV"
      },
      "source": [
        "\n",
        "### SchemaGen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D9GZT1v3lkx"
      },
      "source": [
        "\n",
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'])\n",
        "\n",
        "context.run(schema_gen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW6Vhxy-3lkx"
      },
      "source": [
        "# Visualize the output\n",
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pUvj-IgYQIL"
      },
      "source": [
        "\n",
        "### updating the Schema by TFDV\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYbSyyBrlD8F"
      },
      "source": [
        "schema_uri = schema_gen.outputs['schema']._artifacts[0].uri"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27woxdsF3lky"
      },
      "source": [
        "# Get the schema pbtxt file from the SchemaGen output\n",
        "schema = tfdv.load_schema_text(os.path.join(schema_uri, 'schema.pbtxt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40wynuTzstP3"
      },
      "source": [
        "# Set the two `Hillshade` features to have a range of 0 to 255\n",
        "tfdv.set_domain(schema, 'Hillshade_9am', schema_pb2.IntDomain(name='Hillshade_9am', min=0, max=255))\n",
        "tfdv.set_domain(schema, 'Hillshade_Noon', schema_pb2.IntDomain(name='Hillshade_Noon', min=0, max=255))\n",
        "\n",
        "# Set the `Slope` feature to have a range of 0 to 90\n",
        "tfdv.set_domain(schema, 'Slope', schema_pb2.IntDomain(name='Slope', min=0, max=90))\n",
        "\n",
        "# Set `Cover_Type` to categorical having minimum value of 0 and maximum value of 6\n",
        "tfdv.set_domain(schema, 'Cover_Type', schema_pb2.IntDomain(name='Cover_Type', min=0, max=6, is_categorical=True))\n",
        "\n",
        "tfdv.display_schema(schema=schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHN_j-d5mCCZ"
      },
      "source": [
        "\n",
        "### Schema Environments (training & serving environments)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxyJHhoSl4-3"
      },
      "source": [
        "serving_dir = f'{data_dir}/serving'\n",
        "serving_file = f'{serving_dir}/serving_dataset.csv'\n",
        "\n",
        "!mkdir -p {serving_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpSqjjXeafHr"
      },
      "source": [
        "serving_data = pd.read_csv(training_file, nrows=100)\n",
        "\n",
        "serving_data.drop(columns='Cover_Type', inplace=True)\n",
        "\n",
        "serving_data.to_csv(serving_file, index=False)\n",
        "\n",
        "del serving_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpgqThxImAso"
      },
      "source": [
        "schema.default_environment.append('TRAINING')\n",
        "schema.default_environment.append('SERVING')\n",
        "\n",
        "tfdv.get_feature(schema, 'Cover_Type').not_in_environment.append('SERVING')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsDf37to4SoV"
      },
      "source": [
        "# Declare StatsOptions to use the curated schema\n",
        "stats_options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
        "\n",
        "# Compute the statistics of the serving dataset\n",
        "serving_stats = tfdv.generate_statistics_from_csv(serving_file, stats_options=stats_options)\n",
        "\n",
        "\n",
        "anomalies = tfdv.validate_statistics(serving_stats, schema=schema, environment='SERVING')\n",
        "tfdv.display_anomalies(anomalies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXnDWoGoxsmc"
      },
      "source": [
        "# Declare the path to the updated schema directory\n",
        "updated_schema_dir = f'{pipe_dir}/updated_schema'\n",
        "\n",
        "# Create the said directory\n",
        "!mkdir -p {updated_schema_dir}\n",
        "\n",
        "# Declare the path to the schema file\n",
        "schema_file = f'{updated_schema_dir}/schema.pbtxt'\n",
        "\n",
        "# Save the curated schema to the said file\n",
        "tfdv.write_schema_text(schema, schema_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNvH6V6yxtUm"
      },
      "source": [
        "# Load the schema from the directory we just created\n",
        "new_schema = tfdv.load_schema_text(schema_file)\n",
        "\n",
        "tfdv.display_schema(schema=new_schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxK7GqSgybrb"
      },
      "source": [
        "new_schema.default_environment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npuw7JwMyQ6I"
      },
      "source": [
        "\n",
        "### Generate new statistics using the updated schema & ImporterNode\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhaE8ha13lk0"
      },
      "source": [
        "user_schema_importer = ImporterNode(\n",
        "    source_uri=updated_schema_dir,\n",
        "    artifact_type=standard_artifacts.Schema)\n",
        "\n",
        "context.run(user_schema_importer, enable_cache=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kSOVGAAPwPy"
      },
      "source": [
        "context.show(user_schema_importer.outputs['result'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR1SG-KYnBTR"
      },
      "source": [
        "\n",
        "####  Statistics with the new schema\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5HmdXKKjXF-"
      },
      "source": [
        "\n",
        "statistics_gen_updated = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=user_schema_importer.outputs['result'])\n",
        "\n",
        "context.run(statistics_gen_updated)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e1f6cH2jiPD"
      },
      "source": [
        "context.show(statistics_gen_updated.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx3kqz0CnEqr"
      },
      "source": [
        "\n",
        "#### ExampleValidator for checking anomalies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "graded": true,
        "id": "kvBzLuyPqboL",
        "name": "training_anomalies"
      },
      "source": [
        "### START CODE HERE ###\n",
        "\n",
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen_updated.outputs['statistics'],\n",
        "    schema=user_schema_importer.outputs['result'])\n",
        "    \n",
        "context.run(example_validator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdBbjraG2PZf"
      },
      "source": [
        "# Visualize the results\n",
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duwFgpTYIaYD"
      },
      "source": [
        "\n",
        "####  Feature engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz5VLpMF0R_s"
      },
      "source": [
        "# Set the constants module filename\n",
        "_cover_constants_module_file = 'cover_constants.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOkTSEia0UE3"
      },
      "source": [
        "%%writefile {_cover_constants_module_file}\n",
        "\n",
        "SCALE_MINMAX_FEATURE_KEYS = [\n",
        "        \"Horizontal_Distance_To_Hydrology\",\n",
        "        \"Vertical_Distance_To_Hydrology\",\n",
        "    ]\n",
        "\n",
        "SCALE_01_FEATURE_KEYS = [\n",
        "        \"Hillshade_9am\",\n",
        "        \"Hillshade_Noon\",\n",
        "        \"Horizontal_Distance_To_Fire_Points\",\n",
        "    ]\n",
        "\n",
        "SCALE_Z_FEATURE_KEYS = [\n",
        "        \"Elevation\",\n",
        "        \"Slope\",\n",
        "        \"Horizontal_Distance_To_Roadways\",\n",
        "    ]\n",
        "\n",
        "VOCAB_FEATURE_KEYS = [\"Wilderness_Area\"]\n",
        "\n",
        "HASH_STRING_FEATURE_KEYS = [\"Soil_Type\"]\n",
        "\n",
        "LABEL_KEY = \"Cover_Type\"\n",
        "\n",
        "# Utility function for renaming the feature\n",
        "def transformed_name(key):\n",
        "    return key + '_xf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elp0jej91iiT"
      },
      "source": [
        "# Set the transform module filename\n",
        "_cover_transform_module_file = 'cover_transform.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "graded": true,
        "id": "PJpPxzh6kdNM",
        "name": "exercise_10"
      },
      "source": [
        "%%writefile {_cover_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "import cover_constants\n",
        "\n",
        "_SCALE_MINMAX_FEATURE_KEYS = cover_constants.SCALE_MINMAX_FEATURE_KEYS\n",
        "_SCALE_01_FEATURE_KEYS = cover_constants.SCALE_01_FEATURE_KEYS\n",
        "_SCALE_Z_FEATURE_KEYS = cover_constants.SCALE_Z_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_KEYS = cover_constants.VOCAB_FEATURE_KEYS\n",
        "_HASH_STRING_FEATURE_KEYS = cover_constants.HASH_STRING_FEATURE_KEYS\n",
        "_LABEL_KEY = cover_constants.LABEL_KEY\n",
        "_transformed_name = cover_constants.transformed_name\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "\n",
        "    features_dict = {}\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    for feature in _SCALE_MINMAX_FEATURE_KEYS:\n",
        "        data_col = inputs[feature] \n",
        "        # Transform using scaling of min_max function\n",
        "        # Hint: Use tft.scale_by_min_max by passing in the respective column\n",
        "        features_dict[_transformed_name(feature)] = tft.scale_by_min_max(data_col)\n",
        "\n",
        "    for feature in _SCALE_01_FEATURE_KEYS:\n",
        "        data_col = inputs[feature] \n",
        "        # Transform using scaling of 0 to 1 function\n",
        "        # Hint: tft.scale_to_0_1\n",
        "        features_dict[_transformed_name(feature)] = tft.scale_to_0_1(data_col)\n",
        "\n",
        "    for feature in _SCALE_Z_FEATURE_KEYS:\n",
        "        data_col = inputs[feature] \n",
        "        # Transform using scaling to z score\n",
        "        # Hint: tft.scale_to_z_score\n",
        "        features_dict[_transformed_name(feature)] = tft.scale_to_z_score(data_col)\n",
        "\n",
        "    for feature in _VOCAB_FEATURE_KEYS:\n",
        "        data_col = inputs[feature] \n",
        "        # Transform using vocabulary available in column\n",
        "        # Hint: Use tft.compute_and_apply_vocabulary\n",
        "        features_dict[_transformed_name(feature)] = tft.compute_and_apply_vocabulary(data_col)\n",
        "\n",
        "    for feature in _HASH_STRING_FEATURE_KEYS:\n",
        "        data_col = inputs[feature] \n",
        "        # Transform by hashing strings into buckets\n",
        "        # Hint: Use tft.hash_strings with the param hash_buckets set to 10\n",
        "        features_dict[_transformed_name(feature)] = tft.hash_strings(data_col, hash_buckets = 10)\n",
        "    \n",
        "    features_dict[_LABEL_KEY] = inputs[_LABEL_KEY]\n",
        "\n",
        "    return features_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq88l0XjkdQI"
      },
      "source": [
        "\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=user_schema_importer.outputs['result'],\n",
        "    module_file=os.path.abspath(_cover_transform_module_file))\n",
        "    \n",
        "context.run(transform, enable_cache=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}