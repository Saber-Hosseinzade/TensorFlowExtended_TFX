{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFX_Tuner_and_Trainer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saber-Hosseinzade/TensorFlowExtended_TFX/blob/main/TFX_Tuner_and_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_MPhjWTvNSr"
      },
      "source": [
        "#### Install packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "source": [
        "# Restart the RunTime after completion of this section\n",
        "!pip install tfx==1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_leAIdFKAxAD"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "from tfx.components import ImportExampleGen\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Transform\n",
        "from tfx.components import Tuner\n",
        "from tfx.components import Trainer\n",
        "\n",
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReV_UXOgCZvx"
      },
      "source": [
        "#### Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNQlwf5_t8Fc"
      },
      "source": [
        "pipe_dir = './pipeline/'\n",
        "dataset_dir = './dataset'\n",
        "temp_dir = './temp'\n",
        "\n",
        "!rm -r {pipe_dir}         # remove directory if exists from previous run\n",
        "!rm -r {dataset_dir}      # remove directory if exists from previous run\n",
        "!rm -r {temp_dir}         # remove directory if exists from previous run\n",
        "\n",
        "!mkdir {dataset_dir}      # create directory\n",
        "!mkdir {pipe_dir}         # create directory\n",
        "\n",
        "data = tfds.load('fashion_mnist', data_dir=temp_dir)\n",
        "\n",
        "tfds_data_path = './temp/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001'\n",
        "!cp {tfds_data_path} {dataset_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1gu2Bbi226z"
      },
      "source": [
        "#### Create Interactive Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeCZ5mAvvlD4"
      },
      "source": [
        "# Initialize the InteractiveContext\n",
        "context = InteractiveContext(pipeline_root=pipe_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQwR6Cex3azZ"
      },
      "source": [
        "#### Create ExampleGen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xolw1d8lvqNW"
      },
      "source": [
        "output = example_gen_pb2.Output(\n",
        "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),\n",
        "        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2),\n",
        "    ]))\n",
        "\n",
        "# Ingest the data through ExampleGen\n",
        "example_gen = ImportExampleGen(input_base=dataset_dir, output_config=output)\n",
        "\n",
        "# Run the component\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIdWfRWGxvHp"
      },
      "source": [
        "# Print split names and URI\n",
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "print(artifact.split_names, artifact.uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os6NhLaY4oB3"
      },
      "source": [
        "#### Create StatisticsGen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVDS4oEIzZ83"
      },
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "\n",
        "context.run(statistics_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D48bfGK95sES"
      },
      "source": [
        "#### Create SchemaGen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UhV3Jr7zp7p"
      },
      "source": [
        "# Run SchemaGen\n",
        "schema_gen = SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
        "context.run(schema_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtS2ZEgCzvAf"
      },
      "source": [
        "# Visualize the results\n",
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_yXqq1y6LR6"
      },
      "source": [
        "#### Create ExampleValidator to detect anomalies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaTJiYPpzzZM"
      },
      "source": [
        "# Run ExampleValidator\n",
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "context.run(example_validator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6YzedBSz5KE"
      },
      "source": [
        "# Visualize the results. There should be no anomalies.\n",
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpUFIO9M6yMH"
      },
      "source": [
        "### Transform\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL4zrcJ7z9K9"
      },
      "source": [
        "transform_path = 'transform.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43xmp2UD0Cc5"
      },
      "source": [
        "%%writefile {transform_path}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "def image_fn(input):\n",
        "\n",
        "    output = tf.image.decode_image(input, channels=1)\n",
        "    output = tf.reshape(output, (28, 28, 1))\n",
        "    output = tf.cast(output, tf.float32)\n",
        "    return output\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "\n",
        "    outputs = {\n",
        "        'image_xf':\n",
        "            tf.map_fn(\n",
        "                fn = image_fn,\n",
        "                elems = tf.squeeze(inputs['image'], axis=1),\n",
        "                fn_output_signature = tf.float32),\n",
        "        'label_xf':\n",
        "            tf.map_fn(\n",
        "                fn = lambda x : tf.cast(x, tf.float32),\n",
        "                elems = inputs['label'],\n",
        "                fn_output_signature = tf.float32)\n",
        "    }\n",
        "    \n",
        "    # scale the pixels from 0 to 1\n",
        "    outputs['image_xf'] = tft.scale_to_0_1(outputs['image_xf'])\n",
        "    \n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qthHA2hO1JST"
      },
      "source": [
        "\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(transform_path))\n",
        "\n",
        "context.run(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZkbL7sO8Y1N"
      },
      "source": [
        "#### Tuner\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE1PLAs_6CVt"
      },
      "source": [
        "# Declare name of module file\n",
        "tuner_path = 'tuner.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0F-XhqVlUDB"
      },
      "source": [
        "%%writefile {tuner_path}\n",
        "\n",
        "# Define imports\n",
        "from kerastuner.engine import base_tuner\n",
        "import kerastuner as kt\n",
        "from tensorflow import keras\n",
        "from typing import NamedTuple, Dict, Text, Any, List\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs, DataAccessor\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "# Declare namedtuple field names\n",
        "TunerFnResult = NamedTuple('TunerFnResult', [('tuner', base_tuner.BaseTuner),\n",
        "                                             ('fit_kwargs', Dict[Text, Any])])\n",
        "\n",
        "# Callback for the search strategy\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "\n",
        "def reader_fn(input):\n",
        "\n",
        "  return tf.data.TFRecordDataset(input, compression_type='GZIP')\n",
        "  \n",
        "\n",
        "def _input_fn(file_pattern, tf_transform_output, num_epochs=None, batch_size=32):\n",
        " \n",
        "  # Get feature specification based on transform output\n",
        "  transformed_feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
        "  \n",
        "  # Create batches of features and labels\n",
        "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "      file_pattern=file_pattern,\n",
        "      batch_size=batch_size,\n",
        "      features=transformed_feature_spec,\n",
        "      reader=reader_fn,\n",
        "      num_epochs=num_epochs,\n",
        "      label_key='label_xf')\n",
        "  \n",
        "  return dataset\n",
        "\n",
        "\n",
        "def model_builder(hp):\n",
        "\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28, 1)))\n",
        "\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "def tuner_fn(fn_args):\n",
        "\n",
        "  # Define tuner search strategy\n",
        "  tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory=fn_args.working_dir,\n",
        "                     project_name='kt_hyperband')\n",
        "\n",
        "  # Load transform output\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "\n",
        "  # Use _input_fn() to extract input features and labels from the train and val set\n",
        "  train_set = _input_fn(fn_args.train_files[0], tf_transform_output)\n",
        "  val_set = _input_fn(fn_args.eval_files[0], tf_transform_output)\n",
        "\n",
        "\n",
        "  return TunerFnResult(\n",
        "      tuner=tuner,\n",
        "      fit_kwargs={ \n",
        "          \"callbacks\":[stop_early],\n",
        "          'x': train_set,\n",
        "          'validation_data': val_set,\n",
        "          'steps_per_epoch': fn_args.train_steps,\n",
        "          'validation_steps': fn_args.eval_steps\n",
        "      }\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqVSc6sS5A1m"
      },
      "source": [
        "from tfx.proto import trainer_pb2\n",
        "\n",
        "# Setup the Tuner component\n",
        "tuner = Tuner(\n",
        "    module_file=tuner_path,\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=500),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=100)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdycQnAR7AvG"
      },
      "source": [
        "# Run the component. This will take around 10 minutes to run.\n",
        "context.run(tuner, enable_cache=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW50JS0d9Hd4"
      },
      "source": [
        "#### Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abSJjDM2ipKS"
      },
      "source": [
        "trainer_path = 'trainer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdgbwOFFihSg"
      },
      "source": [
        "%%writefile {trainer_path}\n",
        "\n",
        "from tensorflow import keras\n",
        "from typing import NamedTuple, Dict, Text, Any, List\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs, DataAccessor\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "def reader_fn(filenames):\n",
        "\n",
        "  return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "  \n",
        "\n",
        "def input_fn(file_pattern, tf_transform_output, num_epochs=None, batch_size=32):\n",
        "\n",
        "  transformed_feature_spec = (\n",
        "      tf_transform_output.transformed_feature_spec().copy())\n",
        "  \n",
        "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "      file_pattern=file_pattern,\n",
        "      batch_size=batch_size,\n",
        "      features=transformed_feature_spec,\n",
        "      reader= reader_fn,\n",
        "      num_epochs=num_epochs,\n",
        "      label_key='label_xf')\n",
        "  \n",
        "  return dataset\n",
        "\n",
        "\n",
        "def model_builder(hp):\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28, 1)))\n",
        "\n",
        "  # Get the number of units from the Tuner results\n",
        "  hp_units = hp.get('units')\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "  # Get the learning rate from the Tuner results\n",
        "  hp_learning_rate = hp.get('learning_rate')\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  # Print the model summary\n",
        "  model.summary()\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def run_fn(fn_args):\n",
        "\n",
        "  # Callback for TensorBoard\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=fn_args.model_run_dir, update_freq='batch')\n",
        "  \n",
        "  # Load transform output\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "  \n",
        "  # Create batches of data good for 10 epochs\n",
        "  train_set = input_fn(fn_args.train_files[0], tf_transform_output, 10)\n",
        "  val_set = input_fn(fn_args.eval_files[0], tf_transform_output, 10)\n",
        "\n",
        "  # Load best hyperparameters\n",
        "  hp = fn_args.hyperparameters.get('values')\n",
        "\n",
        "  # Build the model\n",
        "  model = model_builder(hp)\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(\n",
        "      x=train_set,\n",
        "      validation_data=val_set,\n",
        "      callbacks=[tensorboard_callback]\n",
        "      )\n",
        "  \n",
        "  # Save the model\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0JOuqSKGsoQ"
      },
      "source": [
        "# Setup the Trainer component\n",
        "trainer = Trainer(\n",
        "    module_file=trainer_path,\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    hyperparameters=tuner.outputs['best_hyperparameters'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=['train']),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=['eval']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwM2743um1w3"
      },
      "source": [
        "# Run the component\n",
        "context.run(trainer, enable_cache=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu5Bsn0J9ol3"
      },
      "source": [
        "#### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPqoMMXv5NoY"
      },
      "source": [
        "model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {model_run_artifact_dir}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}